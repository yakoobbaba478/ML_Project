{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yakoobbaba478/ML_Project/blob/master/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UU0EqOzJL-0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#imported all the required libraries\n",
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import layers\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from keras.models import Model, load_model\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import layer_utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "from keras.utils import plot_model\n",
        "from keras.initializers import glorot_uniform\n",
        "import scipy.misc\n",
        "import keras.backend as K\n",
        "from tqdm.autonotebook import tqdm\n",
        "from os import listdir\n",
        "from os.path import join,splitext,basename\n",
        "from imgaug import augmenters as iaa\n",
        "import random\n",
        "import traceback\n",
        "from shutil import rmtree\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import json\n",
        "import csv\n",
        "from collections import Counter\n",
        "from keras.callbacks import Callback\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from IPython.display import Image\n",
        "import warnings\n",
        "# Keras Core\n",
        "from keras.layers.convolutional import MaxPooling2D, Convolution2D, AveragePooling2D\n",
        "from keras.layers import Input, Dropout, Dense, Flatten, Activation\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.merge import concatenate\n",
        "from keras import regularizers\n",
        "from keras import initializers\n",
        "# Utils\n",
        "from keras.utils.layer_utils import convert_all_kernels_in_model\n",
        "from keras.utils.data_utils import get_file\n",
        "from IPython.display import Image, display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HGsgsKFK2tI",
        "colab_type": "code",
        "outputId": "64825a7b-9b2b-40db-9211-90165ff8d85d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#mount google drive on notebook\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRGK3ifqK3IL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#extract files from Image_2.zip\n",
        "# extract the .zip file. remove old directory/files if exists. \n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks/')\n",
        "if(os.path.exists('/content/gdrive/My Drive/Colab Notebooks/Image_1')):\n",
        "    rmtree('/content/gdrive/My Drive/Colab Notebooks/Image_1')\n",
        "!unzip -q Image_1.zip -d Image_1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYWbb8IMNuAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#extract files from Data.zip\n",
        "os.chdir('/content/gdrive/My Drive/Colab Notebooks/Image_1/Image_1')\n",
        "if(os.path.exists('/content/gdrive/My Drive/Colab Notebooks/Image_1/Image_1/Data')):\n",
        "    rmtree('/content/gdrive/My Drive/Colab Notebooks//Image_1/Image_1/Data')\n",
        "!unzip -q Data.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtFsyVByK3aQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#classify given data according to the frequency of occurence.\n",
        "class_freq = {\"1\":[],\"2\":[],\"3\":[],\"4\":[],\"5\":[],\"6\":[]}  \n",
        "\n",
        "def create_json(filename,dest_path):\n",
        "    \"\"\"\n",
        "    Creates json files for each image\n",
        "    Args:\n",
        "    \tfilename: csv path which contains label information of each image\n",
        "    \tdest_path: destination path to store json files.\n",
        "  \n",
        "    \"\"\"\n",
        "    # initializing the titles and rows list \n",
        "    fields = [] \n",
        "    rows = []\n",
        "    \n",
        "\n",
        "    # reading csv file \n",
        "    with open(filename, 'r') as csvfile: \n",
        "        # creating a csv reader object \n",
        "        csvreader = csv.reader(csvfile)       \n",
        "        # extracting field names through first row \n",
        "        # extracting each data row one by one \n",
        "        for row in csvreader: \n",
        "            rows.append(row) \n",
        "        # get total number of rows \n",
        "        print(\"Total no. of rows: \",len(rows)) \n",
        "\n",
        "    fields = rows[0]\n",
        "    rows.remove(fields)\n",
        "    # printing the field names \n",
        "    print('Field names are:',fields ) \n",
        "    #  printing first 5 rows \n",
        "    print('\\nFirst 5 rows are:\\n') \n",
        "    large = 0\n",
        "    arr = {}\n",
        "    data = {}\n",
        "    for row in rows: \n",
        "        col0 = str(row[0])\n",
        "        image_id = int(row[1])\n",
        "        col0 = col0.strip('][')\n",
        "        col0 = col0.replace(\"u\",\"\").replace(\"'\",\"\").replace(\" \",\"\")\n",
        "        col0 = col0.split(\",\")\n",
        "        col0 = [int(i) for i in col0]\n",
        "        for item_id in col0:\n",
        "            if item_id not in arr:\n",
        "                arr[item_id] = 0\n",
        "            arr[item_id] += 1\n",
        "        data[image_id] =  col0\n",
        "        if not os.path.exists(dest_path):\n",
        "            os.makedirs(dest_path)   \n",
        "        with open(os.path.join(dest_path, str(image_id)+'.json'), 'w') as f:\n",
        "            json.dump(data[image_id], f)   \n",
        "\n",
        "    for i in arr.keys():\n",
        "        if arr[i]<=50:\n",
        "            class_freq[\"1\"].append(i)\n",
        "        elif arr[i]<150:\n",
        "            class_freq[\"2\"].append(i)\n",
        "        elif  arr[i]<500:\n",
        "            class_freq[\"3\"].append(i)\n",
        "        elif arr[i]<1500:\n",
        "            class_freq[\"4\"].append(i)\n",
        "        elif arr[i]<3000:\n",
        "            class_freq[\"5\"].append(i)\n",
        "        else:\n",
        "            class_freq[\"6\"].append(i)\n",
        "            \n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjpW1QrZ2E9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    \n",
        "            \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF99RtCAsN00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# csv file path,destination path\n",
        "filename = '/content/gdrive/My Drive/Colab Notebooks/Image_1/Image_1/Input/train_labels.csv'\n",
        "dest_path = '/content/gdrive/My Drive/Colab Notebooks/Image_1/Image_1/Input/train'\n",
        "create_json(filename,dest_path)\n",
        "filename = '/content/gdrive/My Drive/Colab Notebooks/Image_1/Image_1/Input/validation_labels.csv'\n",
        "dest_path = '/content/gdrive/My Drive/Colab Notebooks/Image_1/Image_1/Input/val'\n",
        "create_json(filename,dest_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po71zR4uBwsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#compare image data with json files and remove jsons without images.\n",
        "train_img_path = \"/content/gdrive/My Drive/Colab Notebooks/Image_1/Image_1/Data/train\"\n",
        "train_json_path = '/content/gdrive/My Drive/Colab Notebooks/Image_1/Image_1/Input/train'\n",
        "val_img_path = \"/content/gdrive/My Drive/Colab Notebooks/Image_1/Image_1/Data/validation\"\n",
        "val_json_path = '/content/gdrive/My Drive/Colab Notebooks/Image_1/Image_1/Input/val'\n",
        "train_json_list = os.listdir(train_json_path)\n",
        "val_json_list = os.listdir(val_json_path)\n",
        "\n",
        "for i in train_json_list:\n",
        "    base_name = os.path.splitext(i)[0]\n",
        "\n",
        "    if os.path.exists(os.path.join(train_img_path,base_name+'.jpg')):\n",
        "        pass\n",
        "    else:\n",
        "        os.remove(os.path.join(train_json_path,base_name+'.json'))\n",
        "#         print(\"removed: \",os.path.join(train_json_path,base_name+'.json'))\n",
        "for i in val_json_list:\n",
        "    base_name = os.path.splitext(i)[0]\n",
        "\n",
        "    if os.path.exists(os.path.join(val_img_path,base_name+'.jpg')):\n",
        "        pass\n",
        "    else:\n",
        "        os.remove(os.path.join(val_json_path,base_name+'.json'))\n",
        "#         print(\"removed: \",os.path.join(val_json_path,base_name+'.json'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wU67SRaoCEjc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#apply augmentations according to their class frequency. If class freq is less apply more augmentations.\n",
        "prob_class={\"1\":0.8,\"2\":0.75,\"3\":0.5,\"4\":0.4,\"5\":0.2,\"6\":0.05}\n",
        "def apply_img_aug(image_folder,json_folder,class_freq):\n",
        "    \"\"\"\n",
        "    Augmentations\n",
        "    Args:\n",
        "    \timage_folder: Image data path\n",
        "    \tjson_folder: json files path\n",
        "        class_freq: class frequencies\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "        image_files = os.listdir(image_folder)\n",
        "        json_files = os.listdir(json_folder)\n",
        "        pbar = tqdm(total=len(image_files))\n",
        "\n",
        "\n",
        "        for an_image in image_files:\n",
        "                pbar.update(1)\n",
        "                \n",
        "                labels = 0\n",
        "                try:\n",
        "                    count = 0\n",
        "                    image_name_split = os.path.splitext(an_image)\n",
        "                    extension_name = image_name_split[1]\n",
        "                    base_name = image_name_split[0]\n",
        "                    with open(os.path.join(json_folder,base_name+\".json\"), 'r') as f:\n",
        "                        labels = json.load(f)\n",
        "                    #determine probability based on occurence of classes in current image.\n",
        "                    for label in labels:\n",
        "                        if label in class_freq[\"1\"]:\n",
        "                            count+=1\n",
        "                        elif label in class_freq[\"2\"]:\n",
        "                            count+=2\n",
        "                        elif label in class_freq[\"3\"]:\n",
        "                            count+=3\n",
        "                        elif label in class_freq[\"4\"]:\n",
        "                            count+=4\n",
        "                        elif label in class_freq[\"5\"]:\n",
        "                            count+=5\n",
        "                        elif label in class_freq[\"6\"]:\n",
        "                            count+=6\n",
        "                    res = round(count/len(labels))\n",
        "                    prob = prob_class[str(res)]\n",
        "                    \n",
        "                    \n",
        "                    \n",
        "                    \n",
        "                    image_path = os.path.join(image_folder, an_image)\n",
        "\n",
        "                    img_file = cv2.imread(image_path)\n",
        "                    h, w, c = img_file.shape\n",
        "                    \n",
        "                    #Add noise to the image\n",
        "                    seq_noise = iaa.Sequential([iaa.AdditiveGaussianNoise(scale=0.10*255)])\n",
        "                    #multiply image pixels(changes brightness)\n",
        "                    seq_mul = iaa.Sequential([iaa.Multiply(0.5)])\n",
        "                    #flip image left to right\n",
        "                    seq_fliplr = iaa.Sequential([iaa.Fliplr()])\n",
        "                    #add value to image pixels\n",
        "                    seq_add = iaa.Sequential([iaa.Add(value=10)])\n",
        "                    #rotate image\n",
        "                    seq_rotate = iaa.Sequential([iaa.Rot90(1)])\n",
        "                    \n",
        "                    if(random.random()<prob):\n",
        "                        transformed_img = seq_noise.augment_image(img_file)\n",
        "                        transformed_image_base = base_name + '_noise'\n",
        "                        transformed_image_path = os.path.join(image_folder, transformed_image_base +extension_name)\n",
        "                        data = json.load(open(os.path.join(json_folder,base_name+'.json')))\n",
        "                        json.dump(data, open(os.path.join(json_folder,transformed_image_base+'.json'), \"w\"))\n",
        "                        img = cv2.cvtColor(transformed_img, cv2.COLOR_RGB2BGR)\n",
        "                        cv2.imwrite(transformed_image_path, img)\n",
        "                    if(random.random()<prob):          \n",
        "                        transformed_img = seq_mul.augment_image(img_file)\n",
        "                        transformed_image_base = base_name + '_mul'\n",
        "                        transformed_image_path = os.path.join(image_folder, transformed_image_base  +extension_name)\n",
        "                        data = json.load(open(os.path.join(json_folder,base_name+'.json')))\n",
        "                        json.dump(data, open(os.path.join(json_folder,transformed_image_base+'.json'), \"w\"))\n",
        "                        img = cv2.cvtColor(transformed_img, cv2.COLOR_RGB2BGR)\n",
        "                        cv2.imwrite(transformed_image_path, img)\n",
        "                    if(random.random()<prob):\n",
        "                        transformed_img = seq_fliplr.augment_image(img_file)\n",
        "                        transformed_image_base = base_name + '_fliplr'\n",
        "                        transformed_image_path = os.path.join(image_folder, transformed_image_base  +extension_name)\n",
        "                        data = json.load(open(os.path.join(json_folder,base_name+'.json')))\n",
        "                        json.dump(data, open(os.path.join(json_folder,transformed_image_base+'.json'), \"w\"))\n",
        "                        img = cv2.cvtColor(transformed_img, cv2.COLOR_RGB2BGR)\n",
        "                        cv2.imwrite(transformed_image_path, img)\n",
        "                    if(random.random()<prob):                    \n",
        "                        transformed_img = seq_add.augment_image(img_file)\n",
        "                        transformed_image_base = base_name + '_add'\n",
        "                        transformed_image_path = os.path.join(image_folder, transformed_image_base  +extension_name)\n",
        "                        data = json.load(open(os.path.join(json_folder,base_name+'.json')))\n",
        "                        json.dump(data, open(os.path.join(json_folder,transformed_image_base+'.json'), \"w\"))\n",
        "                        img = cv2.cvtColor(transformed_img, cv2.COLOR_RGB2BGR)\n",
        "                        cv2.imwrite(transformed_image_path, img)\n",
        "                    #crop image\n",
        "                    if(random.random()<prob):\n",
        "                        h5,w5 = int(h*0.05),int(w*0.05)\n",
        "                        transformed_img = img_file[h5:h-h5,w5:w-w5]\n",
        "                        transformed_image_base = base_name + '_crop'\n",
        "                        transformed_image_path = os.path.join(image_folder, transformed_image_base  +extension_name)\n",
        "                        data = json.load(open(os.path.join(json_folder,base_name+'.json')))\n",
        "                        json.dump(data, open(os.path.join(json_folder,transformed_image_base+'.json'), \"w\"))\n",
        "                        img = cv2.cvtColor(transformed_img, cv2.COLOR_RGB2BGR)\n",
        "                        cv2.imwrite(transformed_image_path, img)\n",
        "                    if(random.random()<prob):\n",
        "                        transformed_img = seq_rotate.augment_image(img_file)\n",
        "                        transformed_image_base = base_name + '_rot'\n",
        "                        transformed_image_path = os.path.join(image_folder, transformed_image_base  +extension_name)\n",
        "                        data = json.load(open(os.path.join(json_folder,base_name+'.json')))\n",
        "                        json.dump(data, open(os.path.join(json_folder,transformed_image_base+'.json'), \"w\"))\n",
        "                        img = cv2.cvtColor(transformed_img, cv2.COLOR_RGB2BGR)\n",
        "                        cv2.imwrite(transformed_image_path, img)\n",
        "                except Exception as e:\n",
        "                    traceback.print_exc()\n",
        "                    pass\n",
        "        pbar.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALEIO-1ACZd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#image_folder path, json_folder path\n",
        "image_folder = '/content/gdrive/My Drive/Colab Notebooks/Image_1/Image_1/Data/train'\n",
        "json_folder = '/content/gdrive/My Drive/Colab Notebooks/Image_1/Image_1/Input/train'\n",
        "apply_img_aug(image_folder,json_folder,class_freq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3nrRL4HeuF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    from os import scandir\n",
        "except ImportError:\n",
        "    from scandir import scandir\n",
        "def scantree(path):\n",
        "    \"\"\"Recursively yield DirEntry objects for given directory.\"\"\"\n",
        "    for entry in scandir(path):\n",
        "        if entry.is_dir(follow_symlinks=False):\n",
        "            yield from scantree(entry.path)  # see below for Python 2.x\n",
        "        else:\n",
        "            yield entry\n",
        "    \n",
        "    \n",
        "data = {}\n",
        "json_folder = '/content/gdrive/My Drive/Colab Notebooks/Image_1/Image_1/Input/train'\n",
        "\n",
        "pbar = tqdm(total=50000)\n",
        "for entry in scantree(json_folder):\n",
        "    pbar.update(1)\n",
        "    with open(os.path.join(json_folder,entry), 'r') as f:\n",
        "        labels = json.load(f)\n",
        "        for l in labels:\n",
        "            if l not in data:\n",
        "                data[l] = 0\n",
        "            data[l] +=1\n",
        "pbar.close()\n",
        "for i in sorted(data.keys()):\n",
        "    print (i,\"---\",data[i])\n",
        "    \n",
        "          "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax6U11nBJvec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reference:https://github.com/kentsommer/keras-inceptionV4\n",
        "def conv2d_bn(x, nb_filter, num_row, num_col,\n",
        "              padding='same', strides=(1, 1), use_bias=False):\n",
        "    \"\"\"\n",
        "    Utility function to apply conv + BN. \n",
        "    (Slightly modified from https://github.com/fchollet/keras/blob/master/keras/applications/inception_v3.py)\n",
        "    \"\"\"\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        channel_axis = 1\n",
        "    else:\n",
        "        channel_axis = -1\n",
        "    x = Convolution2D(nb_filter, (num_row, num_col),\n",
        "                      strides=strides,\n",
        "                      padding=padding,\n",
        "                      use_bias=use_bias,\n",
        "                      kernel_regularizer=regularizers.l2(0.00004),\n",
        "                      kernel_initializer=initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='normal', seed=None))(x)\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.9997, scale=False)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVQSyKMdJvqX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def block_inception_a(input):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        channel_axis = 1\n",
        "    else:\n",
        "        channel_axis = -1\n",
        "\n",
        "    branch_0 = conv2d_bn(input, 96, 1, 1)\n",
        "\n",
        "    branch_1 = conv2d_bn(input, 64, 1, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 96, 3, 3)\n",
        "\n",
        "    branch_2 = conv2d_bn(input, 64, 1, 1)\n",
        "    branch_2 = conv2d_bn(branch_2, 96, 3, 3)\n",
        "    branch_2 = conv2d_bn(branch_2, 96, 3, 3)\n",
        "\n",
        "    branch_3 = AveragePooling2D((3,3), strides=(1,1), padding='same')(input)\n",
        "    branch_3 = conv2d_bn(branch_3, 96, 1, 1)\n",
        "\n",
        "    x = concatenate([branch_0, branch_1, branch_2, branch_3], axis=channel_axis)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDYi-X9MJvtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def block_reduction_a(input):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        channel_axis = 1\n",
        "    else:\n",
        "        channel_axis = -1\n",
        "\n",
        "    branch_0 = conv2d_bn(input, 384, 3, 3, strides=(2,2), padding='valid')\n",
        "\n",
        "    branch_1 = conv2d_bn(input, 192, 1, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 224, 3, 3)\n",
        "    branch_1 = conv2d_bn(branch_1, 256, 3, 3, strides=(2,2), padding='valid')\n",
        "\n",
        "    branch_2 = MaxPooling2D((3,3), strides=(2,2), padding='valid')(input)\n",
        "\n",
        "    x = concatenate([branch_0, branch_1, branch_2], axis=channel_axis)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmNqtQd1Nc8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def block_inception_b(input):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        channel_axis = 1\n",
        "    else:\n",
        "        channel_axis = -1\n",
        "\n",
        "    branch_0 = conv2d_bn(input, 384, 1, 1)\n",
        "\n",
        "    branch_1 = conv2d_bn(input, 192, 1, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 224, 1, 7)\n",
        "    branch_1 = conv2d_bn(branch_1, 256, 7, 1)\n",
        "\n",
        "    branch_2 = conv2d_bn(input, 192, 1, 1)\n",
        "    branch_2 = conv2d_bn(branch_2, 192, 7, 1)\n",
        "    branch_2 = conv2d_bn(branch_2, 224, 1, 7)\n",
        "    branch_2 = conv2d_bn(branch_2, 224, 7, 1)\n",
        "    branch_2 = conv2d_bn(branch_2, 256, 1, 7)\n",
        "\n",
        "    branch_3 = AveragePooling2D((3,3), strides=(1,1), padding='same')(input)\n",
        "    branch_3 = conv2d_bn(branch_3, 128, 1, 1)\n",
        "\n",
        "    x = concatenate([branch_0, branch_1, branch_2, branch_3], axis=channel_axis)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD83JovtNdRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def block_reduction_b(input):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        channel_axis = 1\n",
        "    else:\n",
        "        channel_axis = -1\n",
        "\n",
        "    branch_0 = conv2d_bn(input, 192, 1, 1)\n",
        "    branch_0 = conv2d_bn(branch_0, 192, 3, 3, strides=(2, 2), padding='valid')\n",
        "\n",
        "    branch_1 = conv2d_bn(input, 256, 1, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 256, 1, 7)\n",
        "    branch_1 = conv2d_bn(branch_1, 320, 7, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 320, 3, 3, strides=(2,2), padding='valid')\n",
        "\n",
        "    branch_2 = MaxPooling2D((3, 3), strides=(2, 2), padding='valid')(input)\n",
        "\n",
        "    x = concatenate([branch_0, branch_1, branch_2], axis=channel_axis)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59lg8kSnNoJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def block_inception_c(input):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        channel_axis = 1\n",
        "    else:\n",
        "        channel_axis = -1\n",
        "\n",
        "    branch_0 = conv2d_bn(input, 256, 1, 1)\n",
        "\n",
        "    branch_1 = conv2d_bn(input, 384, 1, 1)\n",
        "    branch_10 = conv2d_bn(branch_1, 256, 1, 3)\n",
        "    branch_11 = conv2d_bn(branch_1, 256, 3, 1)\n",
        "    branch_1 = concatenate([branch_10, branch_11], axis=channel_axis)\n",
        "\n",
        "\n",
        "    branch_2 = conv2d_bn(input, 384, 1, 1)\n",
        "    branch_2 = conv2d_bn(branch_2, 448, 3, 1)\n",
        "    branch_2 = conv2d_bn(branch_2, 512, 1, 3)\n",
        "    branch_20 = conv2d_bn(branch_2, 256, 1, 3)\n",
        "    branch_21 = conv2d_bn(branch_2, 256, 3, 1)\n",
        "    branch_2 = concatenate([branch_20, branch_21], axis=channel_axis)\n",
        "\n",
        "    branch_3 = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(input)\n",
        "    branch_3 = conv2d_bn(branch_3, 256, 1, 1)\n",
        "\n",
        "    x = concatenate([branch_0, branch_1, branch_2, branch_3], axis=channel_axis)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3buj11r_NoeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Network architecture.\n",
        "def inception_v4_base(input):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        channel_axis = 1\n",
        "    else:\n",
        "        channel_axis = -1\n",
        "\n",
        "    # Input Shape is 299 x 299 x 3 (th) or 3 x 299 x 299 (th)\n",
        "    net = conv2d_bn(input, 32, 3, 3, strides=(2,2), padding='valid')\n",
        "    net = conv2d_bn(net, 32, 3, 3, padding='valid')\n",
        "    net = conv2d_bn(net, 64, 3, 3)\n",
        "\n",
        "    branch_0 = MaxPooling2D((3,3), strides=(2,2), padding='valid')(net)\n",
        "\n",
        "    branch_1 = conv2d_bn(net, 96, 3, 3, strides=(2,2), padding='valid')\n",
        "\n",
        "    net = concatenate([branch_0, branch_1], axis=channel_axis)\n",
        "\n",
        "    branch_0 = conv2d_bn(net, 64, 1, 1)\n",
        "    branch_0 = conv2d_bn(branch_0, 96, 3, 3, padding='valid')\n",
        "\n",
        "    branch_1 = conv2d_bn(net, 64, 1, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 64, 1, 7)\n",
        "    branch_1 = conv2d_bn(branch_1, 64, 7, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 96, 3, 3, padding='valid')\n",
        "\n",
        "    net = concatenate([branch_0, branch_1], axis=channel_axis)\n",
        "\n",
        "    branch_0 = conv2d_bn(net, 192, 3, 3, strides=(2,2), padding='valid')\n",
        "    branch_1 = MaxPooling2D((3,3), strides=(2,2), padding='valid')(net)\n",
        "\n",
        "    net = concatenate([branch_0, branch_1], axis=channel_axis)\n",
        "\n",
        "    # 35 x 35 x 384\n",
        "    # 4 x Inception-A blocks\n",
        "    for idx in range(4):\n",
        "    \tnet = block_inception_a(net)\n",
        "\n",
        "    # 35 x 35 x 384\n",
        "    # Reduction-A block\n",
        "    net = block_reduction_a(net)\n",
        "\n",
        "    # 17 x 17 x 1024\n",
        "    # 7 x Inception-B blocks\n",
        "    for idx in range(7):\n",
        "    \tnet = block_inception_b(net)\n",
        "\n",
        "    # 17 x 17 x 1024\n",
        "    # Reduction-B block\n",
        "    net = block_reduction_b(net)\n",
        "\n",
        "    # 8 x 8 x 1536\n",
        "    # 3 x Inception-C blocks\n",
        "    for idx in range(3):\n",
        "    \tnet = block_inception_c(net)\n",
        "\n",
        "    return net"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWuv9u1kOIai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global model\n",
        "\n",
        "def inception_v4(num_classes, dropout_keep_prob, weights, include_top):\n",
        "    '''\n",
        "    Creates the inception v4 network\n",
        "    Args:\n",
        "    \tnum_classes: number of classes\n",
        "    \tdropout_keep_prob: float, the fraction to keep before final layer.\n",
        "    \n",
        "    Returns: \n",
        "    \tlogits: the logits outputs of the model.\n",
        "    '''\n",
        "\n",
        "    # Input Shape is 299 x 299 x 3 (tf) or 3 x 299 x 299 (th)\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        inputs = Input((3, 299, 299))\n",
        "    else:\n",
        "        inputs = Input((299, 299, 3))\n",
        "\n",
        "    # Make inception base\n",
        "    x = inception_v4_base(inputs)\n",
        "\n",
        "\n",
        "    # Final pooling and prediction\n",
        "    if include_top:\n",
        "        # 1 x 1 x 1536\n",
        "        x = AveragePooling2D((8,8), padding='valid')(x)\n",
        "        x = Dropout(dropout_keep_prob)(x)\n",
        "        x = Flatten()(x)\n",
        "        # 1536\n",
        "        x = Dense(units=num_classes, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs, x, name='inception_v4')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPgX7wVSJvx2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(num_classes=228, dropout_prob=0.2, weights=None, include_top=True):\n",
        "    return inception_v4(num_classes, dropout_prob, weights, include_top)\n",
        "model = create_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcRzXP7YJwMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define custom loss function.\n",
        "\n",
        "import functools\n",
        "def prn_loss_cls(y_true, y_pred):\n",
        "       return K.sum(K.binary_crossentropy(y_true,y_pred), axis=-1)\n",
        "    \n",
        "top5_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=5)\n",
        "top5_acc.__name__ = 'top5_acc'\n",
        "optim = keras.optimizers.Adam(lr=0.001)\n",
        "model.compile(optimizer=optim, loss=prn_loss_cls, metrics=['acc',top5_acc])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8BnO0jiN_eD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W7WjU0lJwiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_input(path):\n",
        "    try:\n",
        "        img = cv2.imread(path)\n",
        "        return(img)\n",
        "    except:\n",
        "        return None\n",
        "def get_output(path,label_dir):\n",
        "    try:\n",
        "        labels = ()\n",
        "        ground_truth = np.zeros(229)\n",
        "        img_id = os.path.splitext(os.path.basename(path))[0]\n",
        "        json_path = os.path.join(label_dir,str(img_id)+\".json\")\n",
        "        with open(json_path, 'r') as f:\n",
        "            labels = json.load(f)\n",
        "        #print(labels) \n",
        "        ground_truth[labels]+=1\n",
        "        ground_truth = ground_truth[1:]\n",
        "        #print(\"lables: \",labels)\n",
        "        return(ground_truth)\n",
        "    except:\n",
        "        return None\n",
        "def preprocess_input(x=None):\n",
        "    input_img = x/255\n",
        "    input_img = cv2.resize(input_img,(299, 299))\n",
        "    return input_img\n",
        "\n",
        "\n",
        "def image_generator(img_list,img_dir,label_dir, batch_size = 8):\n",
        "    \"\"\"\n",
        "    ImageDataGenerator\n",
        "    parameters\n",
        "        img_list: list of image data\n",
        "        img_dir: image data directory\n",
        "        label_dir: json files directory\n",
        "        batch_size: batch size\n",
        "    \"\"\"\n",
        "    \n",
        "    while True:          \n",
        "        # Select files (paths/indices) for the batch\n",
        "        batch_paths = np.random.choice(a = img_list, \n",
        "                                        size = batch_size+2)\n",
        "        batch_input = []\n",
        "        batch_output = [] \n",
        "        count = 0\n",
        "        # Read in each input, perform preprocessing and get labels          \n",
        "        for input_path in batch_paths:              \n",
        "            input_img = get_input(os.path.join(img_dir,input_path) )\n",
        "            if input_img is None:\n",
        "                continue\n",
        "            ground_truth = get_output(input_path,label_dir )\n",
        "            if ground_truth is None:\n",
        "                continue\n",
        "            input_img = preprocess_input(x=input_img)\n",
        "            #print(input_img.shape)\n",
        "            batch_input.append(input_img)\n",
        "            batch_output.append(ground_truth)         \n",
        "        # Return a tuple of (input,output) to feed the network \n",
        "        #print(\"batch_input: \",len(batch_input))\n",
        "        #print(\"bath_output: \",len(batch_output))        \n",
        "        batch_x = np.array( batch_input )\n",
        "        batch_y = np.array( batch_output )\n",
        "        #print(\"batchx[0]: \",batch_x[0])\n",
        "        #print(\"batchy[0]: \",batch_y[0])\n",
        "#         print(\"batch_x: \",batch_x.shape)\n",
        "#         print(\"bath_y: \",batch_y.shape)\n",
        "        \n",
        "        \n",
        "        yield( batch_x, batch_y )\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKXeHcoqJwP1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_path,label_dir, val_path, checkpoint_path):\n",
        "    \"\"\"train\n",
        "    Parameters: \n",
        "   train_path(str): directory path to train images with subfolders as classes.\n",
        "   val_path(str): directory path to validation images with subfolders as classes.\n",
        "   checkpoint_path(str): directory path save checkpoints \n",
        "  \"\"\"\n",
        "\n",
        "    img_list = os.listdir(train_path)\n",
        "    n = len(img_list)\n",
        "    train_set = int(n*0.80)\n",
        "    val_img_list = []\n",
        "    train_img_list = random.sample(img_list,train_set)\n",
        "    for i in img_list:\n",
        "        if i not in train_img_list:\n",
        "            val_img_list.append(i)\n",
        "    training_set = image_generator(train_img_list,train_path,label_dir, batch_size = 8)\n",
        "    validation_set = image_generator(val_img_list,train_path,label_dir,batch_size = 8)\n",
        "    \n",
        "    \n",
        "    checkpoint = keras.callbacks.ModelCheckpoint(checkpoint_path, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=True, mode='auto', period=1)\n",
        "        \n",
        "    callbacks_list = [checkpoint]\t\n",
        "\n",
        "    model.fit_generator(training_set, steps_per_epoch = 3000, epochs = 30,callbacks=callbacks_list, validation_data = validation_set, validation_steps = 10)\n",
        "    #print (\"after model.fit\")\n",
        "\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKMmn8K1Jvof",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_path = '/content/gdrive/My Drive/Colab Notebooks/Image_1/Image_1/Data/train'\n",
        "label_dir = '/content/gdrive/My Drive/Colab Notebooks/Image_1/Image_1/Input/train'\n",
        "checkpoint_dir = '/content/gdrive/My Drive/Colab Notebooks/classifier_weights2/'\n",
        "val_path = '/content/gdrive/My Drive/Colab Notebooks/Image_1/Image_1/Data/validation'\n",
        "checkpoint_path = '/content/gdrive/My Drive/Colab Notebooks/classifier_weights2/weights.{epoch:02d}_{val_acc:.3f}.h5'\n",
        "train(train_path,label_dir,val_path,checkpoint_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcmY15tSA5Nq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "os.path.exists('/content/gdrive/My Drive/Colab Notebooks/Image_1/Image_1/Data/train')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJY9I_4LJvmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#selecting best model from classifier_directory\n",
        "#Loading the best model\n",
        "checkpoint_dir = '/content/gdrive/My Drive/Colab Notebooks/classifier_weights2/'\n",
        "model_list = os.listdir(checkpoint_dir)\n",
        "print(model_list)\n",
        "best_model_path = ''\n",
        "best_score = 0.0\n",
        "for m in model_list:\n",
        "    m_name = os.path.splitext(m)[0]\n",
        "    m_acc = float(m_name.split('_')[1])\n",
        "    if(m_acc>best_score):\n",
        "        best_model_path = m\n",
        "        best_score = m_acc\n",
        "        \n",
        "best_model = os.path.join(checkpoint_dir,best_model_path)\n",
        "model.load_weights(best_model)\n",
        "print(\"best model: \",best_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJUEXrHGJvh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#evaluate best model on test set\n",
        "test_path = '/content/gdrive/My Drive/Colab Notebooks/Image_1/Image_1/Data/validation'\n",
        "label_dir = '/content/gdrive/My Drive/Colab Notebooks/Image_1/Image_1/Input/val'\n",
        "test_img_list = os.listdir(test_path)\n",
        "print(os.path.exists('/content/gdrive/My Drive/Colab Notebooks/Image_1/Image_1/Data/validation'))\n",
        "print(\"testing started........\",len(test_img_list))\n",
        "\n",
        "p_bar = tqdm(total=len(test_img_list))\n",
        "print (len(test_img_list[:800]))\n",
        "for i in random.sample(test_img_list[:2000],800):\n",
        "    p_bar.update(1)\n",
        "    correct = 0\n",
        "    precisions = []\n",
        "    ground_truth = []\n",
        "    img_path = os.path.join(test_path,i)\n",
        "    name = os.path.basename(img_path)[0]\n",
        "    test_img = cv2.imread(img_path)\n",
        "    test_img = test_img/255\n",
        "    test_img = cv2.resize(test_img, (299, 299))\n",
        "    test_img = np.array([test_img])\n",
        "    json_path = os.path.join(label_dir,name+'.json')\n",
        "    with open(json_path, 'r') as f:\n",
        "        ground_truth = json.load(f)\n",
        "    ground_truth = np.array(ground_truth)\n",
        "#     print(name,\"----->\",ground_truth)\n",
        "    output = model.predict(test_img)\n",
        "#     print (output)\n",
        "#     output = np.nonzero(output > 0.5)\n",
        "    output = np.where(output>=0.5)\n",
        "    output = output[1]\n",
        "    output = [i+1 for i in output]\n",
        "#     print (\"output:\",sorted(output))\n",
        "#     print (\"ground:\",sorted(ground_truth))\n",
        "#     display(Image(filename=img_path))\n",
        "#     break\n",
        "    for i in output:\n",
        "        if i in ground_truth:\n",
        "            correct += 1\n",
        "    precision = correct/len(ground_truth)\n",
        "    precisions.append(precision)\n",
        "    \n",
        "p_bar.close()\n",
        "#     print(\"output--->\",output)\n",
        "#     print(precisions)\n",
        "average_precision = sum(precisions)/len(precisions)\n",
        "print(\"average_precision: \",average_precision)\n",
        "print(\"testing ended............\")\n",
        "\n",
        "\n",
        "# confusion_matrix = np.zeros((4,4))\n",
        "# p_bar = tqdm(total=len(test_img_list))\n",
        "# for idx in range(len(test_img_list)):\n",
        "#     x = test_set[idx]\n",
        "#     p_bar.update(1)\n",
        "#     test_image = x[0]\n",
        "#     test_image_label = x[1]\n",
        "#     print(test_image,\"----->\",test_image_label)\n",
        "#     truth = np.argmax(test_image_label)\n",
        "#     output = model.predict(test_image)\n",
        "#     print(\"output---->\",output)\n",
        "#     break\n",
        "#     output_idx = np.argmax(output)\n",
        "#     confusion_matrix[truth][output_idx]+=1\n",
        "# p_bar.close()\n",
        "    \n",
        "# print(\"confusion matrix:\")\n",
        "# print(confusion_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLUwGT4hx4rG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}